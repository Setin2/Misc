{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9KxGZWMQm8XPW/b/BA3XB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import wrappers\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from collections import defaultdict\n",
        "import itertools"
      ],
      "metadata": {
        "id": "IW5oOsj3MQL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEJnGTQVCJiQ"
      },
      "outputs": [],
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = []\n",
        "        self.previous_board = []\n",
        "\n",
        "    def reset(self):\n",
        "      self.board = []\n",
        "      for i in range(3):\n",
        "          row = []\n",
        "          for j in range(3):\n",
        "              row.append(-1)\n",
        "          self.board.append(row)\n",
        "      return self.board\n",
        "\n",
        "    def step(self, row, col, player):\n",
        "      if self.board[row][col] == -1:\n",
        "        self.previous_board = self.board\n",
        "        self.board[row][col] = player\n",
        "        # done and this player won, +1 reward\n",
        "        if self.done(player):\n",
        "          return self.board, 1, True\n",
        "        # done and its a draw, 0 reward\n",
        "        elif self.is_board_filled():\n",
        "          return self.board, 0, True\n",
        "        # not done, 0 reward\n",
        "        else: return self.board, 0, False\n",
        "      # we tried to change a place on the board that is already filled\n",
        "      else: return self.board, -5, False\n",
        "\n",
        "    def done(self, player):\n",
        "        win = None\n",
        "\n",
        "        n = len(self.board)\n",
        "\n",
        "        # checking rows\n",
        "        for i in range(n):\n",
        "            win = True\n",
        "            for j in range(n):\n",
        "                if self.board[i][j] != player:\n",
        "                    win = False\n",
        "                    break\n",
        "            if win:\n",
        "                return win\n",
        "\n",
        "        # checking columns\n",
        "        for i in range(n):\n",
        "            win = True\n",
        "            for j in range(n):\n",
        "                if self.board[j][i] != player:\n",
        "                    win = False\n",
        "                    break\n",
        "            if win:\n",
        "                return win\n",
        "\n",
        "        # checking diagonals\n",
        "        win = True\n",
        "        for i in range(n):\n",
        "            if self.board[i][i] != player:\n",
        "                win = False\n",
        "                break\n",
        "        if win:\n",
        "            return win\n",
        "\n",
        "        win = True\n",
        "        for i in range(n):\n",
        "            if self.board[i][n - 1 - i] != player:\n",
        "                win = False\n",
        "                break\n",
        "        if win:\n",
        "            return win\n",
        "        return False\n",
        "\n",
        "        for row in self.board:\n",
        "            for item in row:\n",
        "                if item == -1:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    # we check if the other player has won (which means that this one lost)\n",
        "    def player_lost(self, player):\n",
        "      player = 1 if player == 0 else 1\n",
        "      return self.done(player)\n",
        "\n",
        "    def is_board_filled(self):\n",
        "        for row in self.board:\n",
        "            for item in row:\n",
        "                if item == -1:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def get_possible_moves(self, state):\n",
        "        moves = []\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                if state[i][j] == -1:\n",
        "                    moves.append([i, j])\n",
        "        return moves\n",
        "    \n",
        "    def undo(self):\n",
        "        self.board = self.previous_board\n",
        "\n",
        "    def one_index_to_2D(self, index):\n",
        "        indices = [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]]\n",
        "        return indices[index]\n",
        "\n",
        "    def move_to_index(self, move):\n",
        "        indices = [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]]\n",
        "        return indices.index(move)\n",
        "\n",
        "    def render(self):\n",
        "        for row in self.board:\n",
        "            for item in row:\n",
        "                print(item, end=\" \")\n",
        "            print()\n",
        "    \n",
        "    def hash(self, state):\n",
        "        hash_value = 0\n",
        "        for i in self.board:\n",
        "          for j in i:\n",
        "            if j == -1: hash_value = self.append_digit(3, hash_value)\n",
        "            elif j == 0: hash_value = self.append_digit(2, hash_value)\n",
        "            else: hash_value = self.append_digit(1, hash_value)\n",
        "        return hash_value\n",
        "    \n",
        "    def append_digit(self, digit, num): \n",
        "      return int(str(digit) + str(num))  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class N_Step_Q_Learning():\n",
        "  def __init__(self, env, N, epsilon, learning_rate=0.9, discount_factor=0.95, q_init=0.5):\n",
        "        self.env = env\n",
        "        self.player = 0\n",
        "        self.Q = {}  # [string, np.ndarray]\n",
        "        self.move_history = []\n",
        "        self.discount_factor = discount_factor\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.N = N\n",
        "        super().__init__()\n",
        "\n",
        "  \"\"\"\n",
        "    We get the q-values of all the actions given this state\n",
        "    If the state is not in the Q-table, we initialize all the q-values as 0.5\n",
        "  \"\"\"\n",
        "  def get_q_value(self, state):\n",
        "      hashed_state = str(state)\n",
        "      if hashed_state in self.Q:\n",
        "        return self.Q[hashed_state]\n",
        "      else: \n",
        "        q_values = np.full(9, 0.5)\n",
        "        self.Q[hashed_state] = q_values\n",
        "        return q_values\n",
        "\n",
        "  \"\"\"\n",
        "    We get the move that has the best q-value\n",
        "  \"\"\"\n",
        "  def get_move(self, state):\n",
        "      hashed_state = str(state)\n",
        "      q_values = self.get_q_value(state)\n",
        "      while True:\n",
        "        move = np.argmax(q_values)\n",
        "        move = env.one_index_to_2D(move)\n",
        "        if move in self.env.get_possible_moves(state):\n",
        "          return move\n",
        "        else: q_values[self.env.move_to_index(move)] = -1.0\n",
        "\n",
        "  \"\"\"\n",
        "    Randomly select a move from the list of possible moves, or get a move based on learned policy depending on our sample\n",
        "  \"\"\"\n",
        "  def random_epsilon_greedy_policy(self, state):\n",
        "    sample = random.random()\n",
        "    if sample > self.epsilon:\n",
        "      return self.get_move(state)\n",
        "    else:\n",
        "      moves = env.get_possible_moves(state)\n",
        "      hashed_state = str(state)\n",
        "      if hashed_state in self.Q:\n",
        "        return moves[random.randrange(len(moves))]\n",
        "      else: \n",
        "        q_values = np.full(9, 0.5)\n",
        "        self.Q[hashed_state] = q_values\n",
        "        return moves[random.randrange(len(moves))]\n",
        "\n",
        "  def train(self, state):\n",
        "    while True:\n",
        "      if self.player == 0:\n",
        "        move = self.random_epsilon_greedy_policy(state)\n",
        "        self.move_history.append((str(state), self.env.move_to_index(move)))\n",
        "      else: move = self.make_best_move(state, self.player)\n",
        "      next_state, reward, done = self.env.step(move[0], move[1], self.player)\n",
        "\n",
        "      self.player = 0 if self.player == 1 else 1\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "        num_steps = 0\n",
        "        for states, moves in self.move_history:\n",
        "          if num_steps < self.N:\n",
        "            self.Q[states][moves] += 0#self.learning_rate * (reward + self.discount_factor) * np.max(self.Q[str(next_state)][:] - self.Q[str(states)][moves])\n",
        "            num_steps += 1\n",
        "          else: self.Q[states][moves] += 0\n",
        "        self.env.render()\n",
        "        print()\n",
        "        break\n",
        "  \n",
        "  \"\"\"\n",
        "    This is the expert policy.\n",
        "    We use the minmax algorithm to get teh score of each move and return the move with max score\n",
        "  \"\"\"\n",
        "  def make_best_move(self, state, player):\n",
        "    bestScore = -math.inf\n",
        "    bestMove = None\n",
        "    env_clone = copy.deepcopy(self.env)\n",
        "    for move in env_clone.get_possible_moves(state):\n",
        "        env_clone.step(move[0], move[1], player)\n",
        "        score = self.minimax(False, player, env_clone)\n",
        "        env_clone.undo()\n",
        "        if (score > bestScore):\n",
        "            bestScore = score\n",
        "            bestMove = move\n",
        "    return bestMove\n",
        "\n",
        "  \"\"\"\n",
        "    In short this algorithm makes a search tree for the given state and returns a score\n",
        "  \"\"\"\n",
        "  def minimax(self, isMaxTurn, player, env):\n",
        "      state = env.board\n",
        "      # draw\n",
        "      if (env.is_board_filled()):\n",
        "          return 0\n",
        "      # this player won\n",
        "      elif (env.done(player)):\n",
        "          return 1\n",
        "      # this player lost\n",
        "      elif (env.player_lost(player)): \n",
        "          return -1\n",
        "\n",
        "      scores = []\n",
        "      for move in env.get_possible_moves(state):\n",
        "          env.step(move[0], move[1], player)\n",
        "          scores.append(self.minimax(not isMaxTurn, player, env))\n",
        "          env.undo()\n",
        "\n",
        "      return max(scores) if isMaxTurn else min(scores)\n",
        "\n",
        "env = TicTacToe()\n",
        "agent = N_Step_Q_Learning(env, N=1, epsilon=0.1)\n",
        "\n",
        "for e in range(10):\n",
        "  state = env.reset()\n",
        "  agent.train(state)"
      ],
      "metadata": {
        "id": "8Wz1RmPtNcrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c072f16-7162-4d18-9f0e-0009c0cec08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1 0 \n",
            "1 0 1 \n",
            "0 -1 -1 \n",
            "\n",
            "1 0 1 \n",
            "0 1 0 \n",
            "1 -1 -1 \n",
            "\n",
            "0 1 1 \n",
            "0 0 1 \n",
            "0 -1 -1 \n",
            "\n",
            "1 0 1 \n",
            "0 1 0 \n",
            "1 -1 -1 \n",
            "\n",
            "0 1 0 \n",
            "1 0 1 \n",
            "0 -1 -1 \n",
            "\n",
            "1 1 0 \n",
            "0 1 0 \n",
            "1 0 1 \n",
            "\n",
            "0 1 1 \n",
            "0 -1 -1 \n",
            "0 -1 -1 \n",
            "\n",
            "1 0 1 \n",
            "0 1 0 \n",
            "1 -1 -1 \n",
            "\n",
            "0 1 0 \n",
            "1 0 1 \n",
            "0 -1 -1 \n",
            "\n",
            "1 0 1 \n",
            "0 1 0 \n",
            "1 -1 -1 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}